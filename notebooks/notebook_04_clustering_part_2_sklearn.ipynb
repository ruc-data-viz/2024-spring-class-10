{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40549c20-3b1f-4b43-a275-0c4ef1f1fe82",
   "metadata": {},
   "source": [
    "# Basic Clustering\n",
    "\n",
    "Grouping data by some simple category is not always feasible or practical. Sometimes we have data that is numerical with no clear means of grouping. Numerically grouping data is what we refer to as clustering, and there are many algorithms and routines to accomplish it.\n",
    "\n",
    "## K-Means Clustering\n",
    "\n",
    "Clustering algorithms apply a statistical process to a set of points that usually includes computing some sort of metric. This metric could be based on distances between points or centroids, density of regions, or even based on distributions of the data. There are many other general methods for performing clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8be067-111d-4425-82c6-5fce8bd57d0f",
   "metadata": {},
   "source": [
    "\n",
    "We are going to start simple, and implement and test out some methods based on centroid distances. We are going to start with some dummy data provided to us by `sklearn`.\n",
    "\n",
    "We use the `make_blobs` function to create a set of 600 data points with 5 clusters. When we visualize the data set we can loosely see the clusters. However, at the boundaries of the clusters that are adjacent, it is difficult to determine which points belong to which cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c80a9-3d8a-41ec-bbe0-0ad34a3e23c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=600, centers=5, random_state=2)\n",
    "_data = X.reshape((600,2)).T\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'x': _data[0],\n",
    "    'y': _data[1]\n",
    "})\n",
    "\n",
    "ax = data.plot.scatter(x='x', y='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4e4cc-1fd1-49b6-9855-20d133ba89aa",
   "metadata": {},
   "source": [
    "To partition the data into clusters, we are going to use the `k-means` clustering algorithm. This is a centroid based method that iteratively groups points by their distances to centroids within the data space. To start this off, we need some initialize our algorithm with some some centroids. We are going to randomly select 5 points without our set and chose them as the centroids. This is called the `Forgy` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31fdfc4-8bf6-4602-b4fc-af3dab387afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "_centroids = X[np.random.randint(0,600, size=5)]\n",
    "centroids = _centroids.reshape((5,2)).T\n",
    "ax.scatter(centroids[0], centroids[1], c=['purple', 'red', 'orange', 'yellow', 'lime'])\n",
    "ax.figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94de4ba8-5537-49b6-9c02-4e882fb0e2be",
   "metadata": {},
   "source": [
    "These 5 points (we selected 5, since we expect there to be 5 clusters found) are just randomly chosen. This means that our final cluster assignment is likely to be suboptimal, and furthermore it is not guaranteed to run quickly (though this is generally true).\n",
    "\n",
    "Regardless, we can proceed. Next we need to then determine which centroid is closest for each point. We are going to define a new function to help compute the closest centroid.\n",
    "\n",
    "This function `find_closet_centroid` returns the index of the centroid it is closest to. It uses `numpy's` function `np.linalg.norm` to compute the distances, and then uses `np.argmin` to find the index of the smallest distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21482a7-fb4f-49c6-a064-beaf0682dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closet_centroid(point):\n",
    "    distances = [np.linalg.norm(centroid-(point.x, point.y)) for centroid in _centroids]\n",
    "    return np.argmin(distances)\n",
    "\n",
    "data['closest_centroid'] = data.apply(find_closet_centroid, axis=1)\n",
    "data.groupby('closest_centroid').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b690f-9d21-4428-901c-ba3084307d93",
   "metadata": {},
   "source": [
    "We can see that our cluster assignment is not very uniform, and in fact appears be fairly lopsided. We can create a custom colormap and then color each point based on the centroid it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c777923-f041-416f-adde-910e508d95c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "cmap = mpl.colors.ListedColormap(\n",
    "    ['purple', 'red', 'orange', 'yellow', 'lime']\n",
    ")\n",
    "data.plot.scatter(x='x', y='y', c='closest_centroid', cmap=cmap, colorbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cec127-ef35-41f2-9785-56b309213550",
   "metadata": {},
   "source": [
    "This is not so great though as we know that our clusters should be fairly uniform, and the cluster in the bottom right is meant to be a single cluster. We need to iterate, recomputing centroids based on the new clusters found.\n",
    "\n",
    "We create a new function `compute_centroid` that just takes the means of x and y values of each cluster. We use `groupby` to grab all of the points found in each cluster and then overwrite the centroids list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece5d12-d98e-4060-bdad-2f0f455a9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroid(cluster):\n",
    "    return np.array([cluster.x.mean(), cluster.y.mean()])\n",
    "    \n",
    "for cidx, cluster in data.groupby('closest_centroid'):\n",
    "    _centroids[cidx] = compute_centroid(cluster)\n",
    "\n",
    "centroids = _centroids.reshape((5,2)).T\n",
    "_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14d9364-106b-472a-93a9-4f5320cfa007",
   "metadata": {},
   "source": [
    "We can replot our initial dataset with the new centroids to see how things changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c7f13-b7ec-44d8-a3ed-15de33899fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = data.plot.scatter(x='x', y='y')\n",
    "ax.scatter(centroids[0], centroids[1], c=['pink', 'red', 'orange', 'yellow', 'lime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023aa76b-5cb7-4574-a76e-fedfb9c89c43",
   "metadata": {},
   "source": [
    "And if we recolor our points..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc0f6b-0d43-448d-b7f3-e14f3359d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['closest_centroid'] = data.apply(find_closet_centroid, axis=1)\n",
    "data.plot.scatter(x='x', y='y', c='closest_centroid', cmap=cmap, colorbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26561d06-bb5d-456a-bc83-0a9416e18cee",
   "metadata": {},
   "source": [
    "After just one iteration our cluster assignments are looking much better. We can continue to iterate a few more times and we can see that just a few more iterations gives us a pretty good cluster assignment. The general process involves iterating until the shift in centroids is less than some threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1e14e-9f3b-4347-8e95-7bae62a265f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    for cidx, cluster in data.groupby('closest_centroid'):\n",
    "        _centroids[cidx] = compute_centroid(cluster)\n",
    "    data['closest_centroid'] = data.apply(find_closet_centroid, axis=1)\n",
    "data.plot.scatter(x='x', y='y', c='closest_centroid', cmap=cmap, colorbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360c9587-ed31-4066-8f6a-c215feec3c4c",
   "metadata": {},
   "source": [
    "Another initialization method called the `Random Partition` method requires us to assign a random cluster to each our points and compute our initial centroids that way. From there we iterate all the same. This suffers from similar performance issues.\n",
    "\n",
    "However, there is no need to actually perform this manually. We can use `sklearn` to compute this for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f99d4-21da-4b1c-bef8-925fc451bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=2).fit_predict(X)\n",
    "data.plot.scatter(x='x', y='y', c=kmeans, cmap=cmap, colorbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57e60dc-ec07-4ef8-bd0f-3880f3d589cb",
   "metadata": {},
   "source": [
    "`sklearn` provides a number of clustering algorithm implementations - different algorithms have different use cases, fidelity, and performance. Check them out [here](https://scikit-learn.org/stable/modules/clustering.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
